<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <title>lecture note</title>
        <!-- Bootstrap CSS CDN -->
        <link rel="stylesheet" href="css/bootstrap.min.css">
        <!-- Our Custom CSS -->
        <link rel="stylesheet" href="css/style.css">
        <!-- Scrollbar Custom CSS -->
        <link rel="stylesheet" href="css/jquery.mCustomScrollbar.min.css">
        <link rel="stylesheet" href="css/font-awesome.min.css">
        <link rel="stylesheet" href="css/vs2015.min.css">
        <script src="js/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
    </head>
    <body>
        <div class="wrapper">
            <!-- Sidebar Holder -->
            <nav id="sidebar">
                <div class="sidebar-header">
                    <h3 class="korean">김성훈 교수님</h3>
                    <a href="https://www.inflearn.com/course/%EA%B8%B0%EB%B3%B8%EC%A0%81%EC%9D%B8-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%95%EC%A2%8C/" target="_blank" data-toggle="tooltip" title="강의 보러 가기 click me"><h5 class="korean">모두를 위한 머신러닝/딥러닝</h5></a>
                </div>
                <ul class="list-unstyled components">
                    <p class="korean">INDEX</p>
                    <li>
                        <a href="index.html">Intro</a>
                    </li>
                    <li class="active">
                        <a href="#homeSubmenu" data-toggle="collapse" aria-expanded="false">Lect</a>
                        <ul class="korean collapse list-unstyled" id="homeSubmenu">
                            <li><a href="lect01.html">Lect01. 머신러닝의 개념과 용어</a></li>
                            <li><a href="lect02.html">Lect02. Linear Regression의 개념</a></li>
                            <li><a href="lect03.html">Lect03. Linear Regression cost 함수 최소화</a></li>
                            <li><a href="lect04.html">Lect04. 여러개의 입력(feature)의 Linear Regression</a></li>
                            <li><a href="lect05.html">Lect05. Logistic Classification (Regression)</a></li>
                            <li><a href="lect06.html">Lect06. Softmax Regression (Multinomial Logistic Regression)</a></li>
                            <li><a href="lect07.html">Lect07. ML의 실용과 몇 가지 팁</a></li>
                            <li><a href="lect08.html">Lect08. 딥러닝의 기본 개념과 문제, 그리고 해결</a></li>
                            <li><a href="lect09.html">Lect09. Neural Network 1 : XOR 문제와 학습방법, Backpropagation</a></li>
                            <li><a href="lect10.html">Lect10. Neural Network 2 : ReLU and 초기 값 정하기 (2006/2007 breakthrough)</a></li>
                            <li class="active"><a href="lect11.html">Lect11. Convolutional Neural Networks (a.k.a CNN)</a></li>
                            <li><a href="lect12.html">Lect12. Recurrent Neural Network (a.k.a RNN)</a></li>
                            <li><a href="lect13.html">Lect13. Deep Deep Network AWD에서 GPU와 돌려보기 (powered by AWS)</a></li>
                            <li><a href="lect14.html">Lect14. AWS에서 저렴하게 Spot Instance를 터미네이션 걱정 없이 사용하기</a></li>
                            <li><a href="lect15.html">Lect15. Google Cloud ML을 이용해 TensorFlow 실행하기</a></li>
                        </ul>
                    </li>
                    <li>
                        <a href="#pageSubmenu" data-toggle="collapse" aria-expanded="false">TF example</a>
                        <ul class="korean collapse list-unstyled" id="pageSubmenu">
                            <li><a href="lab01.html">Lab 01. TensorFlow의 설치 및 기본적인 operations</a></li>
                            <li><a href="lab02.html">Lab 02. Tensorflow로 간단한 linear regression을 구현</a></li>
                            <li><a href="lab03.html">Lab 03. Linear Regression의 cost 최소화의 TensorFlow 구현</a></li>
                            <li><a href="lab04_1.html">Lab 04_1. Multi-variable Linear Regression을 TensorFlow에서 구현하기</a></li>
                            <li><a href="lab04_2.html">Lab 04_2. TensorFlow로 파일에서 데이타 읽어오기</a></li>
                            <li><a href="lab05.html">Lab 05. TensorFlow로 Logistic Classification 구현하기</a></li>
                            <li><a href="lab06_1.html">Lab 06_1. TensorFlow로 SoftmaxClassification 구현하기</a></li>
                            <li><a href="lab06_2.html">Lab 06_2. TensorFlow로 Fancy Softmax Classification 구현하기</a></li>
                            <li><a href="lab07_1.html">Lab 07_1. training/test dataset, learning rate, normalization</a></li>
                            <li><a href="lab07_2.html">Lab 07_2. Meet MNIST Dataset</a></li>
                            <li><a href="lab08.html">Lab 08. Tensor Manipulation</a></li>
                            <li><a href="lab09_1.html">Lab 09_1. XOR을 위한 TensorFlow Deep Network</a></li>
                            <li><a href="lab09_2.html">Lab 09_2. Tensor Board로 Deep Network 들여다 보기</a></li>
                            <li><a href="lab10.html">Lab 10. 딥러닝으로 MNIST 98%이상 해보기</a></li>
                            <li><a href="lab11_1.html">Lab 11_1. TensorFlow의 CNN 기본</a></li>
                            <li><a href="lab11_2.html">Lab 11_2. TensorFlow로 구현하는 MNIST 99%</a></li>
                            <li><a href="lab11_3.html">Lab 11_3. Class, tf.layers, Ensemble (MNIST 99.5%)</a></li>
                            <li><a href="lab12_1.html">Lab 12_1. RNN - Basic</a></li>
                            <li><a href="lab12_2.html">Lab 12_2. RNN - Hi Hello Training</a></li>
                            <li><a href="lab12_3.html">Lab 12_3. Long Sequence RNN</a></li>
                            <li><a href="lab12_4.html">Lab 12_4. Stacked RNN + Softmax Layer</a></li>
                            <li><a href="lab12_5.html">Lab 12_5. Dynamic RNN</a></li>
                            <li><a href="lab12_6.html">Lab 12_6. RNN with Time Series Data</a></li>
                        </ul>
                    </li>
                    
                </ul>
                <ul class="list-unstyled CTAs">
                    <li><a href="https://github.com/hunkim/DeepLearningZeroToAll/archive/master.zip" class="download">Download source</a></li>
                    <br>

                    <p>reference</p>
                    <li>
                        <a href="https://www.facebook.com/groups/TensorFlowKR/?fref=nf" target="_blank"><i class="fa fa-facebook-square fa-3x" aria-hidden="true"></i>&nbsp; TensorFlow KR 페이스북</a>
                    </li>
                    <li>
                        <a href="https://tensorflowkorea.gitbooks.io/tensorflow-kr/content/g3doc/api_docs/" target="_blank"><i class="fa fa-book fa-3x" aria-hidden="true"></i>&nbsp; TensorFlow API doc(KR)</a>
                    </li>
                </ul>
            </nav>

            <!-- Page Content Holder -->
            <div id="content">
                <nav class="navbar navbar-default">
                    <div class="container-fluid">
                        <div class="navbar-header">
                            <button type="button" id="sidebarCollapse" class="btn btn-info navbar-btn">
                            <i class="glyphicon glyphicon-align-left"></i>
                            <span class="korean sidebar_flag">사이드바 접기/펼치기</span>
                            </button>
                        </div>
                        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                            <ul class="nav navbar-nav navbar-right">
                                <li id="content1_list" class="selected"><a>ConvNet의 Conv 레이어 만들기</a></li>
                                <li id="content2_list"><a>ConvNet Max pooling과 Full Network</a></li>
                                <li id="content3_list"><a>ConvNet의 활용 예</a></li>
                            </ul>
                        </div>
                    </div>
                </nav>
                <!-- content1 always all content start from index 1 -->
                <div id="content1" hidden="true">
                    <h3>Convolutional Neural Networks <small>CNNs / ConvNets</small></h3><br>
                    <p><big>1959</big>년 고양이에게 특정 그림을 보여주면서 그림에 따라 각기 다른 뉴런들이 반응하는 현상으로 부터 CNN의 아이디어를 얻었다.</p>
                    <img src="img/lect11_3.png" class="img-responsive">
                    <span>source : <a href="https://hunkim.github.io/ml/lec11.pdf" target="_blank" class="text-primary">https://hunkim.github.io/ml/lec11.pdf</a></span><br>
                    <br>
                    <p><kbd>ConvNet architecture</kbd>는 다음의 순서를 따른다.</p>
                    <pre><code>INPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?]*M -&gt; [FC->RELU]*K -&gt; FC</code></pre><br>
                    <p><code>POOL?</code>은 optinal POOLING layer를 사용할지 말지에 대한 option을 뜻한다.</p>
                    <p><code>FC</code>는 Fully Connected의 약자로서 최종단에서 분류된 class를 취합하는데 사용된다</p>
                    <p>그림으로 아래와 같이 나타낼 수 있다.</p>
                    <img src="img/lect11_1.jpeg" class="img-responsive">
                    <span>source : <a href="http://cs231n.github.io/convolutional-networks/" class="text-primary" target="_blank">http://cs231n.github.io/convolutional-networks/</a></span><br>
                    <br>
                    <h3>Start with an image <small>(width x height x depth)</small></h3><br>
                    <img src="img/lect11_4.png" class="img-responsive">
                    <span>source : <a href="https://hunkim.github.io/ml/lec11.pdf" target="_blank" class="text-primary">https://hunkim.github.io/ml/lec11.pdf</a></span><br><br>
                    <p>filter의 크기는 조정 가능하다.</p><br>

                    <h3>Get one Number using the filter</h3>
                    <img src="img/lect11_2.jpeg" class="img-responsive">
                    <span>source : <a href="http://cs231n.github.io/convolutional-networks/#conv" target="_blank" class="text-primary">http://cs231n.github.io/convolutional-networks/#conv</a></span><br><br>
                    <p>하나의 filter는 하나의 값으로 만들 수 있다.</p>
                    <p>one number = <big>Wx+b</big> = <big>ReLU(Wx+b)</big><br>Wx+b로 값을 만들면 linear regression층을,<br>ReLU(Wx+b)로 값을 계산하면 ReLU층을 만들게 되는 것이다.</p><br>

                    <h3>Let's look at other areas with the same filter(w)</h3><br>
                    <img src="img/lect11_5.png" class="img-responsive">
                    <span>source : <a href="https://hunkim.github.io/ml/lec11.pdf" target="_blank" class="text-primary">https://hunkim.github.io/ml/lec11.pdf</a></span><br><br>

                    <p>필터를 옆으로/아래로 옮겨가며 계산한다.</p>
                    <p>그렇다면 얼마나 많은 weight이 필요한것일까?</p><br>

                    <h3>spatial dimensions 관찰하기</h3><br>
                    <p><kbd>W<sub>1</sub> x H<sub>1</sub> x D<sub>1</sub></kbd> 사이즈의 Input Volume을 갖는다고 할 때</p>
                    <ul>
                        <li>
                            <p>filter의 갯수를 <kbd>K</kbd></p>        
                        </li>
                        <li>
                            <p>filter의 spatial extent, 즉 정사각형 길이를 <kbd>F</kbd></p>
                        </li>
                        <li>
                            <p>stride를 <kbd>S</kbd></p>
                        </li>
                        <li>
                            <p>zero padding의 크기를 <kbd>P</kbd>라 하면,</p>
                        </li>
                    </ul>
                    <p><kbd>W<sub>2</sub> x H<sub>2</sub> x D<sub>2</sub></kbd> 사이즈의 output을 만들수 있으며, ouput의 크기는 아래와 같다.</p>
                    <ul>
                        <li>
                            <p style="font-family: times;">W<sub>2</sub> = (W<sub>1</sub> - F + 2P) / S + 1</p>
                        </li>
                        <li>
                            <p style="font-family: times;">H<sub>2</sub> = (H<sub>1</sub> - F + 2P) / S + 1</p>
                        </li>
                        <li>
                            <p style="font-family: times;">D<sub>2</sub> = K</p>
                        </li>
                    </ul>
                    <p>filter 당 <kbd>F &bullet; F &bullet; D<sub>1</sub></kbd> 개의 weights들이 필요하고, total <kbd>F &bullet; F &bullet; D<sub>1</sub> &bullet; K</kbd> 개의 weights과 K개의 bias들이 필요하다.</p>
                    <br>

                    <h4>Example</h4><br>
                    <p><kbd>W<sub>1</sub> = 5, H<sub>1</sub> = 5, D<sub>1</sub> = 3</kbd> 인 input volume에 대하여 CONV layer의 parameter를<br><kbd>K = 2, F = 3, S = 2, P = 1</kbd> 이라 할 때, output volume은<br>(5 - 3 + 2)/2 + 1 = 3으로 <b>3 x 3</b> volume을 갖게 된다.</p><br>
                    <img src="img/lect11_6.png" class="img-responsive">
                    <span>source : <a href="https://hunkim.github.io/ml/lec11.pdf" target="_blank" class="text-primary">https://hunkim.github.io/ml/lec11.pdf</a></span><br><br>
                    <p>교수님께서 강의노트에서 사용한 예제는 Padding layer(=P)가 0이라 할 때,<br><kbd>W<sub>1</sub> = 7, F = 3, S = 1</kbd> 인 경우로 (7 - 3 + 2*0)/1 + 1 = 5,<br>즉 <b>5 x 5</b> volume의 output을 갖게 된다.<br><kbd>S = 2</kbd> 인 경우는 <b>3 x 3</b> volume의 output을 갖게 된다.</p><br>

                    <h4>Padding layer를 갖는 경우</h4><br>
                    <img src="img/lect11_7.png" class="img-responsive">
                    <span>source : <a href="https://hunkim.github.io/ml/lec11.pdf" target="_blank" class="text-primary">https://hunkim.github.io/ml/lec11.pdf</a></span><br><br>
                    <p>padding은 가장자리에 0으로 둘러싸주는 것을 말한다.<br>위의 <kbd>W<sub>2</sub> = (W<sub>1</sub> - F + 2P) / S + 1</kbd> 공식에 따르면 output의 volume은 얼마가 될까?</p>
                    <p>(7 - 3 + 2 * 1) / 1 + 1 = 7, 즉 <b>7 x 7</b> volume의 output이 나옴을 알 수 있다.<br><small>(난 산수를 못해ㅠㅠ 이럴 필요가 없는게 킹 갓 TF가 알아서 다 계산해준다~)</small></p>

                    <div class="line"></div>

                    <h2>Swiping the entire image</h2><br>
                    <img src="img/lect11_8.png" class="img-responsive">
                    <span>source : <a href="https://hunkim.github.io/ml/lec11.pdf" target="_blank" class="text-primary">https://hunkim.github.io/ml/lec11.pdf</a></span><br><br>
                    <p>위와 같은 방법으로 여러개의 filter를 만든다고 하자.</p><br>
                    <img src="img/lect11_9.png" class="img-responsive">
                    <span>source : <a href="https://hunkim.github.io/ml/lec11.pdf" target="_blank" class="text-primary">https://hunkim.github.io/ml/lec11.pdf</a></span><br><br>
                    <p>여러장의 filter로 Convolution Layer를 통과하면 filter 개수의 dimensions을 갖는 map이 생성된다.<br>output volume은 앞서 계산한 방법으로 예측 가능하다. (P = 0 일때 28 x 28)</p>

                    <div class="line"></div>

                    <h2>Convolution layers</h2><br>
                    <img src="img/lect11_10.png" class="img-responsive">
                    <span>source : <a href="https://hunkim.github.io/ml/lec11.pdf" target="_blank" class="text-primary">https://hunkim.github.io/ml/lec11.pdf</a></span><br><br>
                    <p>32 x 32 x 3 의 input image를 6장의 5 x 5 x 3의 filters로 <span class="text-violet">Convolutional ReLU Layer</span>를 통과시키고, 통과한 activation maps을 다시 10장의 5 x 5 x 6의 filters로 <span class="text-violet">Convolutional ReLU Layer</span>를 통과시키는 것을 뜻한다.</p><br>
                    <h4>총 몇개의 weight variables가 필요할까?</h4>
                    <p>우선, 첫 번째 <span class="text-violet">Convolutional ReLU Layer</span>를 통과한 output volume은 28 x 28 x 6임을 알 수 있다. <small class="text-warning">padding이 없고, stride = 1 이란 가정 하에 (32 - 5 + 0) / 1 + 1 = 28</small>
                    <br>두 번째는 위와 같이 계산하면, 24 x 24 x 10이 나온다.</p>

                </div>
                <div id="content2" hidden="true">
                    <h2>Pooling layer <small>sampling</small></h2><br>
                    <img src="img/lect11_11.png" class="img-responsive">
                    <span>source : <a href="https://hunkim.github.io/ml/lec11.pdf" target="_blank" class="text-primary">https://hunkim.github.io/ml/lec11.pdf</a></span><br><br>
                    <p>input image에 대하여 filter를 거치어 만들어진 conv layer에 대하여 개별 layer의 size를 바꾸는 과정을 pooling이라고 한다.</p><br>
                    <img src="img/lect11_12.png" class="img-responsive"><span>source : <a href="https://hunkim.github.io/ml/lec11.pdf" target="_blank" class="text-primary">https://hunkim.github.io/ml/lec11.pdf</a></span><br><br>

                    <h2>Max Pooling</h2><br>
                    <p>가장 큰 값을 골라서 output sampling하기!</p>
                    <img src="img/lect11_13.png" class="img-responsive">

                    <div class="line"></div>

                    <h3>Fully Connected Layer <small>( FC layer )</small></h3><br>
                    <blockquote class="blockquote">
                      <p>Contains neurons that connect to the entire input volume, as in ordinary Neural Networks</p>
                    </blockquote>
                    <br>
                    <p>마지막 pooling을 거치고 난 결과를 사용자가 원하는 만큼의 depth를 거치고나서 softmax classifier를 통과하여 최종 output label이 나오게 된다.</p>
                    <img src="img/lect11_1.jpeg" class="img-responsive">

                    <div class="line"></div>
                    
                    <h3>Demo 보기</h3><br>
                    
                    <a href="https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html" target="_blank" data-toggle="tooltip" title="ConvNetJS demo 보러가기" class="text-primary">ConvNetJS demo : training on CIFAR-10</a>
                    <img src="img/lect11_14.png" class="img-responsive">
                    <span>source : <a href="https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html" target="_blank" class="text-primary">https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html</a></span><br><br>


                </div>
                <div id="content3" hidden="true">
                    <h1>Case Study</h1>
                    <blockquote><small class="mb0">CNN의 활용을 과거 이미지 넷의 우승 모델들로 비추어보자.</small></blockquote><br>

                    <h3>LeNet-5</h3>
                    <span class="text-violet">[LeCun et al., 1998]</span><br><br>
                    <img src="img/lect11_15.png" class="img-responsive"><br>
                    <p>Convolutions에 적용되는 filter는 5 x 5 사이즈며, stride = 1 이다.<br>
                    Subsampling(Pooling) layer는 2 x 2 사이즈며, stride = 2 이다.</p>
                    <p>architecture is <kbd>[CONV-POOL-CONV-POOL-CONV-FC]</kbd></p>

                    <div class="line"></div>

                    <h3>AlexNet</h3>
                    <span class="text-violet">[krizhevsky et al. 2012]</span><br><br>
                    <img src="img/lect11_16.png" class="img-responsive">
                    <span>source : <a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf" target="_blank" class="text-primary">http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf</a></span><br><br>
                    <p>Input : 227 x 227 x 3 <span class="text-danger">R</span><span class="text-success">G</span><span class="text-primary">B</span> images</p>
                    <p><b>First layer</b> (CONV1) : 96@ 11 x 11 filters applied at stride = 4<br>&nbsp; Output volume <b>[55 x 55 x 96]</b><br>&nbsp; Parameters : ( 11 x 11 x 3 ) x 96 = <b>35k</b></p>
                    <p><b>Second layer</b> (POOL1) : 3 x 3 filters applied at stride = 2<br>&nbsp; Output volume : <b>[27 x 27 x 96]</b><br>&nbsp; Parameters : 0!</p>
                    <p>... and ...</p><br>

                    <h4>Full (simplified) AlexNet architecture</h4>
                    <p>[227 x 227 x 3] INPUT<br>
                        [55 x 55 x 96] <span class="text-danger">CONV1</span> : 96@ 11 x 11 filters at stride = 4, pad = 0
                        <br>
                        [27 x 27 x 96] <span class="text-primary">MAX POOL1</span> : 3 x 3 filters at stride = 2
                        <br>
                        [27 x 27 x 96] <span class="text-success">NORM1</span> : Normalization layer
                        <br>
                        [27 x 27 x 256] <span class="text-danger">CONV2</span> : 256@ 5 x 5 filters at stride = 1, pad = 2
                        <br>
                        [13 x 13 x 256] <span class="text-primary">MAX POOL2</span> : 3 x 3 filters at stride = 2
                        <br>
                        [13 x 13 x 256] <span class="text-success">NOMR2</span> : Normalization layer
                        <br>
                        [13 x 13 x 384] <span class="text-danger">CONV3</span> : 384@ 3 x 3 filters at stride = 1, pad = 1
                        <br>
                        [13 x 13 x 384] <span class="text-danger">CONV4</span> : 384@ 3 x 3 filters at stride = 1, pad = 1
                        <br>
                        [13 x 13 x 256] <span class="text-danger">CONV4</span> : 256@ 3 x 3 filters at stride = 1, pad = 1
                        <br>
                        [6 x 6 x 256] <span class="text-primary">MAX POOL3</span> : 3 x 3 filters at stride = 2
                        <br>
                        [4096] <span class="text-warning">FC6</span> : 4096 neurons
                        <br>
                        [4096] <span class="text-warning">FC7</span> : 4096 neurons
                        <br>
                        [4096] <span class="text-warning">FC8</span> : 1000 neurons (class scores)
                        <br>
                        <small><span class="text-primary">p.s.</span>&nbsp;Image Net은 1000개의 레이블을 맞추는 대회이다.</small>
                    </p><br>

                    <h4 class="text-violet">Details/Retrospectives : </h4>
                    <ul>
                        <li>
                            <p>first use of ReLU&nbsp;&nbsp;&nbsp;<span class="text-danger">awesome...</span></p>
                        </li>
                        <li>
                            <p>used Norm layers (not common anymore)</p>
                        </li>
                        <li>
                            <p>heavy data augmentation</p>
                        </li>
                        <li>
                            <p>dropout = 0.5</p>
                        </li>
                        <li>
                            <p>batch size : 128</p>
                        </li>
                        <li>
                            <p>SGD Momentum : 0.9</p>
                        </li>
                        <li>
                            <p>Learning rate = 1e-2, reduced by 10 manually when val accuracy plateaus</p>
                        </li>
                        <li>
                            <p>L2 weight decay 5e-4</p>
                        </li>
                        <li>
                            <p>7 CNN ensemble : 18.2% -&gt; 15.4%</p>
                        </li>
                    </ul>

                    <div class="line"></div>

                    <h3>GoogLeNet</h3>
                    <span class="text-violet">[Szegedy et al. 2014]</span><br><br>
                    <a href="img/lect11_17_origin.png" target="_blank" data-toggle="tooltip" title="원본보기"><img src="img/lect11_17.png" class="img-responsive"></a>
                    <span>source : <a href="https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf" target="_blank" class="text-primary">https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf</a></span><br><br>
                    <p>2014년 ILSVRC winner <span class="text-primary">(6.7% top 5 error)</span></p><br>
                    <p>Inception Module을 처음으로 사용한 모델이다.<br>Inception이란 작은 Conv 레이어 여러개로 연산 density를 높히는 방식</p>
                    <img src="img/lect11_18.png" class="img-responsive">
                    <span>source : <a href="https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf" target="_blank" class="text-primary">https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf</a></span>

                    <div class="line"></div>

                    <h3>ResNet</h3>
                    <span class="text-violet">[He et al., 2015]</span><br><br>

                    <h4>MSRA <small>Microsoft Research Asia</small> @ ILSVRC &amp; COCO 2015 Competitions</h4><br>
                    <h5><span class="text-danger">1st places</span> in all five main tracks</h5>
                    <ul>
                        <li>
                            <p><span class="text-primary">ImageNet Classification : </span><em class="text-black">"Ultra-deep"</em> <small>(quote Yann)</small> <span class="text-danger">152-layer</span> nets</p>
                        </li>
                        <li>
                            <p><span class="text-primary">ImageNet Detection : </span> <span class="text-danger">16%</span> better than 2nd</p>
                        </li>
                        <li>
                            <p><span class="text-primary">ImageNet Localization : </span> <span class="text-danger">27%</span> better than 2nd</p>
                        </li>
                        <li>
                            <p><span class="text-primary">COCO Detection : </span> <span class="text-danger">11%</span> better than 2nd</p>
                        </li>
                        <li>
                            <p><span class="text-primary">COCO Segmentation : </span> <span class="text-danger">12%</span> better than 2nd</p>
                        </li>
                    </ul><br>

                    <p>2015년 ILSVRC winner <span class="text-primary">(3.6% top 5 error)</span></p><br>

                    <h4>Revolution of Depth</h4>
                    <br>
                    <img src="img/lect11_19.png" class="img-responsive"><br>
                    <ul>
                        <li>
                            <p>8개의 GPU로 2~3주간 학습 시간 소요</p>
                        </li>
                        <li>
                            <p>8배 이상의 layer를 갖고도 VGGNet보다 runtime이 빠름</p>
                        </li>
                    </ul><br>

                    <h4>forward pass 방식 사용.</h4><br>
                    <img src="img/lect11_20.png" class="img-responsive"><span>source : <a href="http://image-net.org/challenges/talks/ilsvrc2015_deep_residual_learning_kaiminghe.pdf" target="_blank" class="text-primary">http://image-net.org/challenges/talks/ilsvrc2015_deep_residual_learning_kaiminghe.pdf</a></span>

                    <div class="line"></div>

                    <h4>Convolutional Neural Networks for Sentence Clssification</h4>
                    <span class="text-violet">[Yoon Kim, 2014]</span><br><br>
                    <img src="img/lect11_21.png" class="img-responsive">
                    <span>source : <a href="http://www.aclweb.org/anthology/D14-1181" target="_blank" class="text-primary">http://www.aclweb.org/anthology/D14-1181</a></span><br><br>
                    <p>text에 대한 분류도 CNN을 활용한 논문이 있다.</p>

                    <div class="line"></div>

                    <h3>DeepMind's AlphaGo</h3><br>
                    <p>2016년 3월 15일 바둑의 역사를 새로 쓴 알파고 역시 CNN을 사용하였다.</p><br>
                    <img src="img/google-deepmind-go.jpg" class="img-responsive">
                    <span>source : <a href="https://www.cnet.com/news/google-deepmind-hooked-us-on-go-the-geekiest-game-youve-never-heard-of/" target="_blank" class="text-primary">https://www.cnet.com/news/google-deepmind-hooked-us-on-go-the-geekiest-game-youve-never-heard-of/</a></span><br><br>

                    <h4>part of nature article</h4>
                    <img src="img/lect11_22.png" class="img-responsive">
                    <span>source : <a href="https://www.nature.com/articles/nature16961" target="_blank" class="text-primary">https://www.nature.com/articles/nature16961</a></span><br><br>
                    <p><b>policy network:</b></p>
                    <ul>
                        <li>
                            <p>[19 x 19 x 48] Input</p>        
                        </li>
                        <li>
                            <p>CONV1 : 192@ 5 x 5 filters, stride = 1, pad = 2 <span class="text-violet">=&gt;</span> output : [19 x 19 x 192]</p>
                        </li>
                        <li>
                            <p>CONV2 : 192@ 3 x 3 filters, stride = 1, pad = 1 <span class="text-violet">=&gt;</span> output : [19 x 19 x 192]</p>
                        </li>
                        <li>
                            <p>Final CONV : 1@ 1 x 1 filters, stride = 1, pad = 0 <span class="text-violet">=&gt;</span> output : [19 x 19] <small><em>(probability map of promising moves)</em></small></p>
                        </li>
                    </ul>
                </div>

                <div class="footer">
                    <ul class="pager">
                        <li class="previous" id="prev_button" style="color:#00BFFF;"><a href="#">Previous</a></li>
                        <li class="next" id="next_button" style="color:#00BFFF;"><a href="#">Next</a></li>
                    </ul>
                </div>
            </div>
        </div>
        
        <!-- jQuery CDN -->
        <script src="js/jquery-1.12.0.min.js"></script>
        <!-- Bootstrap Js CDN -->
        <script src="js/bootstrap.min.js"></script>
        <!-- jQuery Custom Scroller CDN -->
        <script src="js/jquery.mCustomScrollbar.concat.min.js"></script>
        
        <script type="text/javascript">
        $(document).ready(function () {
        $("#sidebar").mCustomScrollbar({
        theme: "minimal"
        });
        $('#sidebarCollapse').on('click', function () {
        $('#sidebar, #content').toggleClass('active');
        $('.collapse.in').toggleClass('in');
        $('a[aria-expanded=true]').attr('aria-expanded', 'false');
        });
        var cnt = 0;
        for(i=1;;i++){
        var local_id = "#content"+i;
        if($(local_id).length==0) break;
        cnt++;
        }
        if(cnt<=1){
        $("#prev_button").hide();
        $("#next_button").hide();
        $(".sidebar_flag").show();
        }
        var current = 1;
        var local_id = "#content"+current;
        $(local_id).show();
        local_id+="_list";
        $(local_id).addClass("selected");
        $("#prev_button").click(function(){
        if(current>1){
        local_id = "#content"+current;
        $(local_id).hide();
        local_id += "_list";
        $(local_id).removeClass("selected");
        current--;
        local_id = "#content"+current;
        $(local_id).show();
        local_id += "_list";
        $(local_id).addClass("selected");
        }
        });
        $("#next_button").click(function(){
        if(current<cnt){
        local_id = "#content"+current;
        $(local_id).hide();
        local_id += "_list";
        $(local_id).removeClass("selected");
        current++;
        local_id = "#content"+current;
        $(local_id).show();
        local_id += "_list";
        $(local_id).addClass("selected");
        }
        });
        $(".recap_question").hover(function(){
            $(this).css("color","red");
            $(this).find(".recap_answer").show();
            }, function(){
            $(this).css("color","black");
            $(this).find(".recap_answer").hide();
        }); 
        $('[data-toggle="tooltip"]').tooltip();
        if($(window).width() <= 1200){
            $("#bs-example-navbar-collapse-1").css("font-size","75%");
        }else{
            $("#bs-example-navbar-collapse-1").css("font-size","100%");
        }
        $(window).resize(function(){
            if($(window).width() <= 1200){
                $("#bs-example-navbar-collapse-1").css("font-size","75%");
            }else{
                $("#bs-example-navbar-collapse-1").css("font-size","100%");
            }
        });
        });
        </script>
    </body>
</html>